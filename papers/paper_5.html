<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Update Title -->
    <title>Paper Summary: Prompt Learning for CLIP - Soham Kumar</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap">
    <!-- Link to the external CSS file -->
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="content-wrapper">

        <a href="../index.html" class="back-link">‚Üê Back to Home</a>

        <header class="page-header">
             <!-- Update H1 -->
            <h1>Paper Summary: Natural Language Supervision in Vision Tasks & CLIP</h1>
            <div class="page-meta">
                 <!-- Optional: Add date or original publication info -->
                 <span>Tags: <small class="tag">Technical</small><small class="tag">Paper</small><small class="tag">ML</small><small class="tag">Vision-Language</small></span>
            </div>
        </header>

        <!-- Original content structured with new styling -->
        <section id="papers-covered">
            <h2>Papers Covered</h2>
            <p>This summary discusses techniques for learning visual representations using natural language supervision, focusing on the ideas presented in several key papers:</p>
            <ul>
                 <li><a href="#" target="_blank" rel="noopener noreferrer"><strong>VirTex:</strong> Learning Visual Representations from Textual Annotations</a> (Link to PDF if available)</li>
                 <li><a href="#" target="_blank" rel="noopener noreferrer"><strong>ICMLM:</strong> Learning Visual Representations with Caption Annotations</a> (Link to PDF if available)</li>
                 <li><a href="#" target="_blank" rel="noopener noreferrer"><strong>CONVirt:</strong> Contrastive Learning of Medical Visual Representations from Paired Images and Text</a> (Link to PDF if available)</li>
                 <li><a href="#" target="_blank" rel="noopener noreferrer"><strong>CLIP:</strong> Learning Transferable Visual Models From Natural Language Supervision</a> (Link to PDF if available)</li>
            </ul>
             <p><em>Note: Placeholder links used. Replace with actual PDF links if you have them.</em></p>
        </section>

        <section id="problem">
            <h2>Problem Solved</h2>
            <p>
                Traditional computer vision relies heavily on large, manually annotated datasets (e.g., ImageNet). While effective, creating these datasets is expensive, time-consuming, and often requires domain expertise (like in medical imaging). Furthermore, the fixed categories limit the model's ability to understand more nuanced or open-ended visual concepts.
            </p>
            <p>
                These papers explore an alternative: leveraging naturally occurring image-text pairs (like images with captions from the web or medical images with reports). This approach aims to learn rich, transferable visual representations without relying on explicit category labels, using the semantic information embedded in the accompanying text as supervision. The core question is: how can we effectively learn from this paired data?
            </p>
        </section>

        <section id="methodologies">
            <h2>Methodologies</h2>

            <h3>VirTex: Text Generation as Pretext Task</h3>
            <p>
                VirTex trains a visual backbone (CNN) and a textual head (Transformer) jointly to generate captions for given images. The intuition is that to generate accurate captions, the visual model must learn semantically meaningful features. After pre-training, the text transformer is discarded, and the visual backbone is used for downstream tasks. They argue captions provide a "semantically denser" signal than simple labels or even unsupervised contrastive learning.
            </p>
            <img src="assets/paper_5/VirTex_architecture.png" alt="VirTex architecture diagram">
            <img src="assets/paper_5/semantic_density.png" alt="Comparison of semantic density in labels vs captions">

            <h3>ICMLM: Multiple Proxy Tasks</h3>
            <p>
                ICMLM explores various ways ("proxy tasks") to learn from image-caption pairs:
            </p>
            <ul>
                <li><strong>Tag Prediction (TP, TPPostag, TPCluster):</strong> Treat concepts/tags extracted from captions (using n-grams, POS tagging, or clustering) as multi-label classification targets for the image.</li>
                <li><strong>Image-Conditioned Masked Language Model (ICMLM):</strong> Predict masked words in a caption using both the image features and the surrounding text context, often fused using attention or transformers (ICMLMtfm, ICMLMatt-fc).</li>
            </ul>
            <p>The visual encoder learned through these tasks is then transferred.</p>
            <img src="assets/paper_5/ICMLM_architecture.png" alt="ICMLM architecture diagram">

            <h3>CONVirt & CLIP: Contrastive Learning</h3>
            <p>
                Both CONVirt (focused on medical images) and CLIP (large-scale web data) employ a contrastive approach. They learn a joint embedding space where paired images and texts have high similarity (cosine similarity), while unpaired combinations have low similarity.
            </p>
            <p>
                Architecture: An image encoder (e.g., ResNet, ViT) and a text encoder (e.g., BERT, Transformer) produce embeddings. During training, for a batch of N image-text pairs, the model computes an N x N similarity matrix. The goal is to maximize similarity along the diagonal (correct pairs) and minimize it off-diagonal (incorrect pairs).
            </p>
            <img src="assets/paper_5/CONVirt_architecture.png" alt="CONVirt architecture diagram">
            <img src="assets/paper_5/CLIP_architecture.png" alt="CLIP architecture and inference diagram">
            <p>
                CLIP's key differentiator was the massive scale (400M pairs) and its demonstration of powerful zero-shot transfer capabilities, allowing the model to perform tasks it wasn't explicitly trained for by phrasing them as image-text matching problems (e.g., classifying an image by comparing it against text prompts like "a photo of a dog", "a photo of a cat").
            </p>

        </section>

        <section id="notes">
            <h2>Personal Notes & Takeaways</h2>
            <ul>
                <li>The shift towards using natural language provides richer supervision than fixed labels, leading to more generalizable representations.</li>
                <li>Contrastive learning (CLIP, CONVirt) seems particularly effective, especially at scale, for learning joint embeddings.</li>
                <li>The success of CLIP highlights the "bitter lesson": large datasets and scalable, general methods (like contrastive learning on transformers) often outperform more complex, specialized approaches in the long run.</li>
                <li>Visualization techniques (attention maps, t-SNE) are crucial for understanding and validating what these models learn.</li>
                <li>Zero-shot capabilities enabled by models like CLIP are transformative, reducing the need for task-specific fine-tuning.</li>
            </ul>
        </section>

        <section id="references">
            <h2>References & Further Reading</h2>
             <!-- Keep original references if relevant, adjust formatting -->
             <p>Primary sources:</p>
             <ul>
                 <li><a href="#" target="_blank" rel="noopener noreferrer">VirTex Paper</a></li>
                 <li><a href="#" target="_blank" rel="noopener noreferrer">ICMLM Paper</a></li>
                 <li><a href="#" target="_blank" rel="noopener noreferrer">CONVirt Paper</a></li>
                 <li><a href="#" target="_blank" rel="noopener noreferrer">CLIP Paper</a></li>
            </ul>
            <p>Related concepts:</p>
             <ul>
                 <li><a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html" target="_blank" rel="noopener noreferrer">The Bitter Lesson by Rich Sutton</a></li>
                 <li><a href="https://arxiv.org/abs/2210.03117" target="_blank" rel="noopener noreferrer">MaPLe Paper (Example of visualization)</a></li>
                 <!-- Add other relevant links -->
            </ul>
             <p><em>Note: Placeholder links used.</em></p>
        </section>

        <footer id="site-footer">
            <p><small>Hosted on <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a></small></p>
        </footer>

    </div> <!-- End content-wrapper -->
</body>
</html>