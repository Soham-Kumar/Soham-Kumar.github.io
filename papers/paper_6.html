<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title> Paper Summary - Soham</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Courier New', monospace;
        }

        body {
            background-color: #0a0a0a;
            color: #e0e0e0;
            line-height: 1.6;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
        }

        h1, h2, h3 {
            color: #00ff9d;
        }

        h1 {
            font-size: 2.5rem;
            margin-bottom: 1rem;
        }

        h2 {
            font-size: 1.8rem;
            margin-top: 2rem;
            margin-bottom: 1rem;
        }

        h3 {
            font-size: 1.4rem;
            margin-top: 1.5rem;
            margin-bottom: 0.5rem;
        }

        p {
            margin-bottom: 1rem;
            color: #ccc;
        }

        .content-block {
            background-color: #111;
            border: 1px solid #333;
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1.5rem 0;
        }

        a {
            color: #00ff9d;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 2rem;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <!-- Paper Title -->
        <section>
            <h1><span style="color: #fff;"> BERT </span></h1>
            <p><a href="papers_pdf/BERT.pdf" target="_blank"><strong>Paper 1:</strong></a> BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding <br>
            <a href="papers_pdf/ModernBERT.pdf" target="_blank"><strong>Paper 2:</strong></a> Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference </p>
            
        <!-- Problem Solved -->
        <section>
            <h2>Problem Solved</h2>
            <p>
                The standard approach to vision related tasks is to take a large manually annotated dataset and train on it. This reaches SOTA and also learns representations useful for downstream tasks. But this approach has major
                limitations. The cost and complexity of annotating datasets is considerable, especially when the class taxonomy is fine-grained requiring expert knowledge. The natural consequence to this problem
                is to either think of data sources that are easily available or to use unsupervised learning methods. Unsupervised learning methods don't directly use the labels of the datasets,
                but are highly data intensive, and even more so when they are deprived of the implicitly benefits from careful selection and the resulting underlying structure of the generally used dataset.</br></br>
                 
                Interestingly, data often comes with informative metadata for free. For instance, user tags associated with images can be used as image labels.
                Even richer, companion text for images, is sometimes available for free. To utilize this type of data, the task is reframed to: <i>Can we train transferable visual representations from limited sets
                of image-caption pairs? If so, how should we formulate the interaction between images and captions?</i> All these 3 papers aim to model this in different ways.
            </p>
        </section>

        <!-- Methodology -->
        <section>
            <h2>Methodology - VirTex</h2>
            <p>
                This paper tries to model the interactions between images and captions by modeling this task as multiple different proxy tasks. All these tasks warrant our attention as they are all pretty intersting desgin problems.
                The paper doesn't make firm claims about why one of the proxies achieve the highest scores. Following are the proxy tasks:
                <ul>
                    <li> </li>
                </ul> 
            </p>
        </section>

        <section>
            <h2>Methodology - CoCoOp</h2>
            <p>
                CoCoOp builds on CoOp by incorporating a lightweight neural network to generate an input-conditional token (vector) for each image.
                Unlike CoOp’s static prompts, which are tailored to each task or class, CoCoOp’s dynamic prompts adapt to each individual instance, making them less sensitive to class shifts.
                In CoOp, the learned context overfits the base classes, failing to capture more generalizable elements needed for broader scene recognition. The static context in CoOp is
                optimized solely for a specific set of training classes. In contrast, <i>CoCoOp conditions the prompt on each input instance (image)</i>, avoiding the overfitting issue of static context.</br></br>

                A simple approach to implement CoCoOp would involve building M neural networks to generate M context tokens, but this would result in a model significantly larger than CoOp due
                to the increased number of parameters (M × neural network size). Instead like the M context vectors from CoOp are learnt using a lightweight neural network, Meta-Net, which generates
                a conditional token for each input. This token is then combined with the context vectors, achieving efficiency while improving the model's adaptability. </br></br>
                <img src="assets/paper_4/CoCoOp.png" alt="CoCoOp architecture" style="max-width: 100%; height: auto;">
                CoCoOp architecture has two learnable parameters - first is the context like CoOp, and the other is the Meta-Net, which generates the conditional token for each input image.

            </p>
        </section>

        <!-- Personal Notes -->
        <section>
            <h2>Personal Notes</h2>
            <p>
                <ul>
                    <li></li>
                    <li></li>
                </ul>
            </p>
        </section>

        <!-- References -->
        <section>
            <h2>References</h2>
            <p>
                This blog is not academic in nature, and the content is taken from a lot of sources. The primary source of information are the <a href="https://arxiv.org/abs/2103.00350" target="_blank"> CoOp</a> and <a href="https://arxiv.org/abs/2203.05557" target="_blank"> CoCoOp</a> papers themselves.
                Apart from them the following sources were used (non-exhaustive list):
                <ul>
                    <li><a href="https://arxiv.org/abs/2103.00020" target="_blank"> Learning Transferable Visual Models From Natural Language Supervision (CLIP)</a></li>
                    
                </p>
        </section>
    </div>
</body>
</html>
